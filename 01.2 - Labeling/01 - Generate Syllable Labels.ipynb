{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 - UMAP Cluster and HDBSCAN Label Syllables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "conda create -n myenv python=3.8\n",
    "conda activate myenv\n",
    "pip install avn\n",
    "conda install -c conda-forge umap-learn\n",
    "conda install -c conda-forge hdbscan\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tkoch\\anaconda3\\envs\\avn_umap_2024\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import avn.dataloading\n",
    "import avn.plotting\n",
    "import hdbscan\n",
    "import math\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import umap\n",
    "import os\n",
    "import glob\n",
    "from random import sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_spec(syll_wav, hop_length, win_length, n_fft, amin, ref_db, min_level_db):\n",
    "    spectrogram = librosa.stft(syll_wav, hop_length = hop_length, win_length = win_length, n_fft = n_fft)\n",
    "    spectrogram_db = librosa.amplitude_to_db(np.abs(spectrogram), amin = amin, ref = ref_db)\n",
    "    \n",
    "    #normalize\n",
    "    S_norm = np.clip((spectrogram_db - min_level_db) / -min_level_db, 0, 1)\n",
    "    \n",
    "    return S_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_syll_table(syll_table): \n",
    "  '''\n",
    "  Reformats syll tables imported from evsonganaly so that they are compatible\n",
    "  with python generated ones\n",
    "\n",
    "  Inputs\n",
    "  ----\n",
    "  syll_table: Pandas Dataframe, imported from a csv containing evsonganaly \n",
    "  segmentation and labeling info\n",
    "\n",
    "  Outputs\n",
    "  ----\n",
    "  syll_table: Pandas Dataframe, now with corrected file names and timestamps in seconds\n",
    "\n",
    "  Notes\n",
    "  -----\n",
    "  This function specifically removes .not.mat file extensions from the file names\n",
    "  so that they are simply .wav and can be compared to file names in the MFCC\n",
    "  segmentation generated syllable tables. It also converts the timestamps of \n",
    "  onsets and offsets from miliseconds to seconds, again so that it is consistent\n",
    "  with the MFCC resuls\n",
    "  ''' \n",
    "\n",
    "  syll_table['onsets'] = syll_table['onsets'] / 1000\n",
    "  syll_table['offsets'] = syll_table['offsets'] / 1000\n",
    "\n",
    "  syll_table['files'] = syll_table['files'].str.split(pat = '.not', n = 1).str[0]\n",
    "\n",
    "  return syll_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(all_matches):\n",
    "  temp_df = pd.DataFrame({\"hand_labels\": all_matches.hand_label, \n",
    "                          \"clusters\": all_matches.hdbscan_labels})\n",
    "  conf_mat = pd.crosstab(temp_df.hand_labels, temp_df.clusters)\n",
    "  conf_mat = conf_mat.div(conf_mat.sum(axis = 1), axis = 0) * 100\n",
    "\n",
    "  plt.figure(figsize = (4,3))\n",
    "  sns.heatmap(conf_mat, annot = True, fmt='.0f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_hdbscan_hand_dfs(hdbscan_df, hand_df):\n",
    "  \n",
    "  all_matches = pd.DataFrame()\n",
    "\n",
    "  for song_file in hdbscan_df.files.unique():\n",
    "    hdbscan_file = hdbscan_df[hdbscan_df.files == song_file]\n",
    "    hdbscan_file = hdbscan_file.reset_index(drop = True)\n",
    "    hand_file = hand_df[hand_df.files == song_file]\n",
    "\n",
    "    if hand_file.shape[0] != 0:\n",
    "      hand_onsets = hand_file.onsets\n",
    "      hdbscan_onsets = hdbscan_file.onsets\n",
    "      best_matches = get_best_matches(hand_onsets, hdbscan_onsets, max_gap = 0.1)\n",
    "      matches_file = pd.DataFrame({\"files\" : song_file,\n",
    "                                 \"best_match\" : best_matches.astype(int), \n",
    "                                 \"hand_onset\" : hand_file.onsets, \n",
    "                                 \"hand_offset\" : hand_file.offsets, \n",
    "                                 \"hand_label\" : hand_file.labels})\n",
    "      matches_file = matches_file.merge(right = hdbscan_file, \n",
    "                                        left_on = 'best_match', \n",
    "                                        how = 'outer', \n",
    "                                        right_index = True)\n",
    "    all_matches = pd.concat([all_matches, matches_file])\n",
    "\n",
    "  return all_matches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_matches(first_onsets, second_onsets, max_gap = 0.1):\n",
    "  '''\n",
    "  Finds the best unique matches between timestamps in two sets, calcuated in different\n",
    "  ways on the same file. These can reflect syllable onset or offset timestamps, \n",
    "  although I refer to them only as onsets for simplicity. \n",
    "\n",
    "  Inputs\n",
    "  ------\n",
    "  first_onsets: Pandas Series, Contains the timestamps in seconds of syllable onsets\n",
    "  calculated with a particular method. \n",
    "\n",
    "  second_onsets: Pandas Seris, Contains the timestamps in seconds of syllable onsets\n",
    "  calculated with a different method. \n",
    "\n",
    "  max_gap: int > 0, optional, maximum allowable time difference  in seconds \n",
    "  between onsets where they will still be considered a match.\n",
    "\n",
    "  Outputs\n",
    "  -------\n",
    "  best_matches: numpy array, 1D, For every syllable onset in `first_onsets` it \n",
    "  contains the index of the best unique match in `second_onsets`. If there is \n",
    "  no unique match within the allowable `max_gap`, the value of the match is \n",
    "  `NaN`. \n",
    "  '''\n",
    "  first_grid, second_grid = np.meshgrid(first_onsets, second_onsets)\n",
    "  delta_t_grid = abs(first_grid - second_grid)\n",
    "\n",
    "  #set max gap threshold\n",
    "  delta_t_grid = np.where(delta_t_grid > max_gap, np.inf, delta_t_grid)\n",
    "\n",
    "  if delta_t_grid.shape[0] == 0:\n",
    "    best_matches = np.array([])\n",
    "    return best_matches ############################################################################test\n",
    "\n",
    "  #find best matches\n",
    "  best_matches = np.argmin(delta_t_grid, axis = 0).astype(float)\n",
    "\n",
    "  #remove matches were delta t is > max_gap\n",
    "  for i, match_index in enumerate(best_matches):\n",
    "    if np.isinf(delta_t_grid[int(match_index), i]):\n",
    "      best_matches[i] = np.NaN \n",
    "\n",
    "  best_matches_previous = best_matches.copy()   \n",
    "\n",
    "  #Deal with duplicate values by setting the second best matches to their second best pairs\n",
    "  best_matches, delta_t_grid = correct_duplicates(best_matches, delta_t_grid)\n",
    "\n",
    "  #check if there were changes made by checking for duplicates. If so, repeat duplicate check.\n",
    "  if not np.allclose(best_matches, best_matches_previous):\n",
    "    best_matches, delta_t_grid = correct_duplicates(best_matches, delta_t_grid)\n",
    "\n",
    "  #make sure duplicate corrections didn't result in out of order matches. \n",
    "  for i, curr_match in enumerate(best_matches):\n",
    "     if i+1 < len(best_matches):\n",
    "       if curr_match > best_matches[i+1]:\n",
    "         best_matches[i+1] = np.nan\n",
    "\n",
    "  return best_matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vak_to_syll_table(prediction_results):\n",
    "  prediction_syllables = prediction_results[prediction_results['label'] == \"s\"]\n",
    "  prediction_syll_reformat = pd.DataFrame()\n",
    "  prediction_syll_reformat[\"files\"] = prediction_syllables['audio_file']\n",
    "  prediction_syll_reformat['onsets'] = prediction_syllables['onset_s']\n",
    "  prediction_syll_reformat['offsets'] = prediction_syllables['offset_s']\n",
    "\n",
    "  return prediction_syll_reformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_duplicates(best_matches, delta_t_grid):\n",
    "  '''\n",
    "  Finds any duplicate matches in the set of best matches, removes duplicates\n",
    "  by setting all but best match to their second best match. \n",
    "\n",
    "  Inputs\n",
    "  -----\n",
    "  best_matches: numpy array, 1D, contains indices of onsets in one set which best \n",
    "  match the onsets in another set. \n",
    "\n",
    "  delta_t_grid: numpy array, 2D, contains all the absolute value time differences\n",
    "  between all possible pairs of onsets in two sets being compared.\n",
    "\n",
    "  Outputs\n",
    "  -----\n",
    "  best_matches: numpy array, 1D. Same as input, but with duplicate matches corrected\n",
    "\n",
    "  delta_t_grid: numpy array, 2D. Same as input, but with values at duplicated \n",
    "  positions adjusted to allow finding second best match. \n",
    "\n",
    "\n",
    "  '''\n",
    "  for i, match_index in enumerate(best_matches):\n",
    "    if np.isnan(match_index):\n",
    "      continue\n",
    "    #check if match index is duplicated\n",
    "    if len(np.argwhere(best_matches == match_index)) > 1: \n",
    "      #create list of indices of duplicates\n",
    "      duplicates = np.nonzero(best_matches == match_index)[0]\n",
    "      #find which duplicate has the smallest delta t\n",
    "      delta_ts = []\n",
    "      for n in duplicates:\n",
    "        delta_ts.append(delta_t_grid[int(best_matches[n]), n])\n",
    "\n",
    "      #get all but the best matches of the duplicates\n",
    "      bad_matches = np.delete(duplicates, np.argmin(delta_ts))\n",
    "\n",
    "      #find second best matches for all but the best duplicate matches\n",
    "\n",
    "      for bad_match in bad_matches:\n",
    "        delta_t_grid[int(best_matches[bad_match]), bad_match] = np.inf\n",
    "        best_matches[bad_match] = np.argmin(delta_t_grid[:, bad_match])\n",
    "        if np.isinf(np.min(delta_t_grid[:, bad_match])):\n",
    "          best_matches[bad_match] = np.NaN\n",
    "  return (best_matches, delta_t_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_nas_from_merge(all_matches, fill_char = 'x', fill_int = 1000):\n",
    "  \n",
    "  all_matches.hand_label = all_matches.hand_label.fillna(fill_char)\n",
    "  all_matches.hdbscan_labels = all_matches.hdbscan_labels.fillna(fill_int)\n",
    "\n",
    "  return all_matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get List of Birds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "All_Birds = [\"B145\", \"B236\", \"B258\", \"B385\", \"B402\", \"B447\", \n",
    "             \"B507\", \"G255\", \"G397\", \"G402\", \"G413\", \"G437\", \n",
    "             \"G439\", \"G524\", \"G528\", \"O144\", \"O254\", \"O421\", \n",
    "             \"O440\", \"O512\", \"R402\", \"R425\", \"R469\", \"S132\", \n",
    "             \"S421\", \"S525\", \"S528\", \"Y389\", \"Y397\", \"Y425\", \n",
    "             \"Y440\", \"B524\", \"O434\", \"Y433\", \"Y453\"] \n",
    "\n",
    "#All_Birds should contain 35 bird IDs. Check: \n",
    "len(All_Birds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "amin = 1e-5\n",
    "ref_db = 20\n",
    "min_level_db = -28\n",
    "win_length = 512 \n",
    "hop_length = 128 \n",
    "n_fft = 512\n",
    "K = 10\n",
    "min_cluster_prop = 0.04\n",
    "embedding_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "870.7482993197278"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(300 * 128 / 44100) * 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B145\n"
     ]
    }
   ],
   "source": [
    "\n",
    "all_v_measures = pd.DataFrame()\n",
    "for Bird_ID in All_Birds[:1]:\n",
    "    print(Bird_ID)\n",
    "    segmentations_path = \"E:\\\\Final_Bird_Dataset\\\\WhisperSeg_Segmentation_labeled\\\\\" + Bird_ID + \"_wseg.csv\"\n",
    "   \n",
    "    song_folder_path = \"E:\\\\Final_Bird_Dataset\\\\FP1_project_birds\\\\labeled_songs\\\\\" + Bird_ID + \"\\\\\"\n",
    "    \n",
    "    output_file = \"E:\\\\Final_Bird_Dataset\\\\UMAP_WSeg_labels_updated\\\\\" + Bird_ID + \"_labels.csv\" \n",
    "\n",
    "    #load segmentations\n",
    "    segmentations = pd.read_csv(segmentations_path)\n",
    "\n",
    "    #load segmentations\n",
    "    segmentations = pd.read_csv(segmentations_path).drop(columns= ['Unnamed: 0'])\n",
    "\n",
    "    segmentations = segmentations.rename(columns = {'onset' : 'onsets', \n",
    "                                                'offset' : 'offsets',\n",
    "                                                'file' : 'files'})\n",
    "    \n",
    "    #Add syllable audio to dataframe\n",
    "    syllable_dfs = pd.DataFrame()\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        for song_file in segmentations.files.unique():\n",
    "            file_path = song_folder_path + song_file\n",
    "            song = avn.dataloading.SongFile(file_path)\n",
    "            song.bandpass_filter(500, 15000)\n",
    "\n",
    "            syllable_df = segmentations[segmentations['files'] == song_file]\n",
    "\n",
    "            #this section is based on avgn.signalprocessing.create_spectrogram_dataset.get_row_audio()\n",
    "            syllable_df[\"audio\"] = [song.data[int(st * song.sample_rate) : int(et * song.sample_rate)]\n",
    "                                    for st, et in zip(syllable_df.onsets.values, syllable_df.offsets.values)]\n",
    "            syllable_dfs = pd.concat([syllable_dfs, syllable_df])\n",
    "        #Normalize the audio\n",
    "        syllable_dfs['audio'] = [librosa.util.normalize(i) for i in syllable_dfs.audio.values]\n",
    "        #compute spectrogram for each syllable\n",
    "        syllables_spec = []\n",
    "\n",
    "        for syllable in syllable_dfs.audio.values:\n",
    "            \n",
    "            syllable_spec = make_spec(syllable, \n",
    "                                        hop_length = hop_length, \n",
    "                                        win_length = win_length, \n",
    "                                        n_fft = n_fft, \n",
    "                                        ref_db = ref_db, \n",
    "                                        amin = amin, \n",
    "                                        min_level_db = min_level_db)\n",
    "            if syllable_spec.shape[1] > 300:\n",
    "                print(\"Long Syllable Corrections! Spectrogram Duration = \" + str(syllable_spec.shape[1]))\n",
    "                syllable_spec = syllable_spec[:, :300]\n",
    "\n",
    "            syllables_spec.append(syllable_spec)\n",
    "            \n",
    "        #normalize spectrograms\n",
    "        def norm(x):\n",
    "            return (x - np.min(x)) / (np.max(x) - np.min(x))\n",
    "\n",
    "        syllables_spec_norm = [norm(i) for i in syllables_spec]\n",
    "        #Pad spectrograms for uniform dimensions\n",
    "        spec_lens = [np.shape(i)[1] for i in syllables_spec]\n",
    "        pad_length = np.max(spec_lens)\n",
    "\n",
    "\n",
    "        syllables_spec_padded = []\n",
    "\n",
    "        for spec in syllables_spec_norm:\n",
    "            to_add = pad_length - np.shape(spec)[1]\n",
    "            pad_left = np.floor(float(to_add) / 2).astype(\"int\")\n",
    "            pad_right = np.ceil(float(to_add) / 2).astype(\"int\")\n",
    "            spec_padded = np.pad(spec, [(0, 0), (pad_left, pad_right)], 'constant', constant_values = 0)\n",
    "            syllables_spec_padded.append(spec_padded)\n",
    "            \n",
    "        #flatten the spectrograms into 1D\n",
    "        specs_flattened = [spec.flatten() for spec in syllables_spec_padded]\n",
    "        specs_flattened_array = np.array(specs_flattened)\n",
    "\n",
    "        #Embed\n",
    "        embedding = umap.UMAP(min_dist = 0.0, random_state = 42).fit_transform(specs_flattened_array)\n",
    "\n",
    "        #cluster\n",
    "        min_cluster_size = math.floor(embedding.shape[0] * min_cluster_prop)\n",
    "        clusterer = hdbscan.HDBSCAN(min_cluster_size = min_cluster_size, core_dist_n_jobs=1, min_samples = 5).fit(embedding)\n",
    "\n",
    "    hdbscan_df = syllable_dfs\n",
    "    hdbscan_df[\"labels\"] = clusterer.labels_\n",
    "    hdbscan_df[\"X\"] = embedding[:, 0]\n",
    "    hdbscan_df[\"Y\"] = embedding[:, 1]\n",
    "    hdbscan_df['hdbscan_labels'] = hdbscan_df.labels\n",
    "    hdbscan_df[\"labels\"] = hdbscan_df['labels'].astype(\"category\")\n",
    "\n",
    "    # hdbscan_df.to_csv(output_file)\n",
    "\n",
    "    # sns.scatterplot(data = hdbscan_df,  x = 'X', y = 'Y', hue  =  'labels', s = 3, alpha = 0.5)\n",
    "    # plt.title(Bird_ID)\n",
    "    # plt.show()\n",
    "\n",
    "    # #load ground truth labels\n",
    "    # hand_label_df_path = \"C:/Grad_School/Code_and_software/Py_code/March_2021_redo/redo_data/labeled_songs/\" + Bird_ID + \"/\" + Bird_ID + \"_syll_df_evsonganaly.csv\"\n",
    "    # hand_label_df = pd.read_csv(hand_label_df_path)\n",
    "    # hand_label_df = clean_syll_table(hand_label_df)\n",
    "    \n",
    "    # #align tweetynet syllables with ground truth syllables\n",
    "    # all_matches = merge_hdbscan_hand_dfs(hdbscan_df, hand_label_df)\n",
    "    # all_matches = remove_nas_from_merge(all_matches)\n",
    "    \n",
    "    # tweetynet_sylls = all_matches[all_matches.hdbscan_labels != 1000]\n",
    "\n",
    "    \n",
    "    # #claculate v-measure\n",
    "    # v_measure = sklearn.metrics.v_measure_score(labels_true = all_matches.hand_label, \n",
    "    #                                            labels_pred = all_matches.hdbscan_labels)\n",
    "    # completeness = sklearn.metrics.completeness_score(labels_true = all_matches.hand_label, \n",
    "    #                                             labels_pred = all_matches.hdbscan_labels)\n",
    "    # homogeneity = sklearn.metrics.homogeneity_score(labels_true = all_matches.hand_label, \n",
    "    #                                           labels_pred = all_matches.hdbscan_labels)\n",
    "    \n",
    "    # curr_v_measure = pd.DataFrame({\"v_measure\" : [v_measure], \n",
    "    #                                \"homogeneity\" : [homogeneity], \n",
    "    #                                \"completeness\" : [completeness],\n",
    "    #                                   \"Bird_ID\" : Bird_ID, \n",
    "    #                                   \"ref_db\": ref_db, \n",
    "    #                                   \"win_length\" : win_length,\n",
    "    #                                   \"hop_length\" : hop_length, \n",
    "    #                                   \"n_fft\" : n_fft, \n",
    "    #                                   \"K_nn\" : K, \n",
    "    #                                   \"min_cluster_prop\" : min_cluster_prop, \n",
    "    #                                   \"embedding_method\" : \"UMAP\", \n",
    "    #                                   \"embedding_dimensions\" : embedding_dim, \n",
    "    #                                   \"min_level_db\" : min_level_db})\n",
    "    \n",
    "    # all_v_measures = pd.concat([all_v_measures, curr_v_measure])\n",
    "    # #plot confusion matrix\n",
    "    # plot_confusion_matrix(all_matches)\n",
    "    # plt.title(Bird_ID + \" V-Measure = \" + str(v_measure))\n",
    "    # plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(460, 44204)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "specs_flattened_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "avn_umap_2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
